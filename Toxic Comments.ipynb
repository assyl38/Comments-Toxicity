{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"_BPmgIO5VEmM"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (2.12.1)\n","Requirement already satisfied: typing-extensions\u003e=4.7.0 in /usr/local/lib/python3.10/dist-packages (from emoji) (4.12.2)\n"]}],"source":["pip install emoji\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fXQeYbSQVGQT"},"outputs":[],"source":["pip install num2words"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rFohhsjiVGTP"},"outputs":[],"source":["pip install pyspellchecker"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dnybdTt4Si9_"},"outputs":[],"source":["import re\n","import string\n","import numpy as np\n","import random\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","\n","from collections import Counter\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","\n","from PIL import Image\n","from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n","\n","from sklearn.model_selection import train_test_split, ParameterGrid\n","\n","# Bag-of words\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# TF-IDF\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","# BOW\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Classifiers\n","from sklearn.naive_bayes import ComplementNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn import svm\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.linear_model import RidgeClassifier\n","\n","from sklearn import metrics\n","from sklearn.metrics import classification_report, accuracy_score,f1_score,confusion_matrix, roc_curve, auc\n","from sklearn.model_selection import StratifiedKFold, GridSearchCV\n","\n","import string, re, nltk\n","import emoji\n","from string import punctuation\n","from nltk.tokenize import word_tokenize, RegexpTokenizer\n","from nltk.corpus import stopwords\n","from num2words import num2words\n","from spellchecker import SpellChecker\n","from nltk.stem.porter import PorterStemmer\n","import spacy\n","from nltk.stem import WordNetLemmatizer\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LTgPKPN5UrI7"},"outputs":[],"source":["# train data\n","train = pd.read_csv(\"/content/train.csv\")\n","train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-SJUgkd3UrLj"},"outputs":[],"source":["train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6bYYm6x3UrOE"},"outputs":[],"source":["train['toxic'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-5NCXzyBUrQz"},"outputs":[],"source":["#test the data without labels\n","test = pd.read_csv('/content/test.csv')\n","test.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IOq08xtpUrTy"},"outputs":[],"source":["#test data labels\n","test_labels = pd.read_csv('/content/test_labels.csv')\n","test_labels.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vp99HlfiUrV8"},"outputs":[],"source":["test_labels['toxic'].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"EFwi0ozGbxUx"},"source":["The target features consist of three values: -1, 0, and 1. To analyze this further, we need to investigate the test data labels. Since this is a binary classification problem, the labels should ideally be 0 or 1. We need to unde"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L-WKap0GUrfi"},"outputs":[],"source":["test_labels[test_labels['toxic']==1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o8AAGj8yb12L"},"outputs":[],"source":["test[test['id']== '0013fed3aeae76b7']['comment_text']\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jWrjicnFb15f"},"outputs":[],"source":["test_labels[test_labels['toxic']==-1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"79SWr0Rdb18d"},"outputs":[],"source":["test[test['id']== '00001cee341fdb12']['comment_text']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NQF09UqWb1-h"},"outputs":[],"source":["test[test['id']== 'ffffce3fb183ee80']['comment_text']"]},{"cell_type":"markdown","metadata":{"id":"M9fYVYE2cdzZ"},"source":["After analyzing the data, we observed that labels with values of -1 and 1 appear to represent the same category. This suggests that -1 might be a data entry error or an incorrect label."]},{"cell_type":"markdown","metadata":{"id":"BYMWHQ5hchkv"},"source":["# ***EDA***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zEnzoToLcZWf"},"outputs":[],"source":["train['comment_len'] = train['comment_text'].apply(lambda x: len(x.split(' ')))\n","train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pr0z9XNDcZZm"},"outputs":[],"source":["train[train['comment_len'].isnull()]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BUrHL4m6cZcE"},"outputs":[],"source":["toxic_df = train[train['toxic']==1].copy()\n","severe_toxic_df = train[train['severe_toxic']==1].copy()\n","obscene_toxic_df = train[train['obscene']==1].copy()\n","insult_df = train[train['insult']==1].copy()\n","identity_hate_df = train[train['identity_hate']==1].copy()\n","non_toxic_df = train[(train['toxic']==0) \u0026\n","                  (train['severe_toxic']==0) \u0026\n","                  (train['obscene']==0) \u0026\n","                  (train['insult']==0) \u0026\n","                 (train['identity_hate']==0)].copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ouwSXMUocZem"},"outputs":[],"source":["# Set Seaborn style\n","sns.set(style='whitegrid')\n","\n","# Create a figure with 2 rows and 2 columns of subplots\n","fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(12, 10))\n","axes = axes.flatten()  # Flatten the 2D array of axes for easy iteration\n","\n","# Define unique colors for each plot\n","colors = ['red','#1fd655']\n","\n","# List of dataframes, their corresponding columns, titles, and colors\n","data_frames = [\n","    (toxic_df, 'toxic', 'Toxic', colors[0]),\n","    (severe_toxic_df, 'severe_toxic', 'Severe Toxic', colors[0]),\n","    (obscene_toxic_df, 'obscene', 'Obscene', colors[0]),\n","    (insult_df, 'insult', 'Insult', colors[0]),\n","    (identity_hate_df, 'identity_hate', 'Identity Hate', colors[0]),\n","    (non_toxic_df, 'toxic', 'non toxic', colors[1]),\n","]\n","\n","# Plot each dataframe in a separate subplot\n","for ax, (df, col, title, color) in zip(axes, data_frames):\n","    sns.boxplot(x=col, y='comment_len', data=df, ax=ax, color=color)\n","    ax.set_title(title, fontsize=14, fontweight='bold')\n","    ax.set_xlabel('')\n","    ax.set_ylabel('Comment Length')\n","    ax.grid(True, linestyle='--', alpha=0.7)\n","\n","# Hide the last subplot if using a 2x3 grid\n","for j in range(len(data_frames), len(axes)):\n","    axes[j].axis('off')\n","\n","# Adjust layout to prevent overlap\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LIHUMQwHcZhP"},"outputs":[],"source":["categories = [\n","    'Toxic',\n","    'Severe Toxic',\n","    'Obscene',\n","    'Insult',\n","    'Identity Hate',\n","    'Non Toxic'\n","]\n","\n","# Corresponding lengths of each dataframe\n","lengths = [\n","    len(toxic_df),\n","    len(severe_toxic_df),\n","    len(obscene_toxic_df),\n","    len(insult_df),\n","    len(identity_hate_df),\n","    len(non_toxic_df)\n","]\n","\n","# Create a DataFrame for plotting\n","data = pd.DataFrame({\n","    'Category': categories,\n","    'Count': lengths\n","})\n","\n","# Set Seaborn style\n","sns.set(style='whitegrid')\n","\n","# Define colors\n","colors = ['#ff4c4c'] * 5  # Red color for all categories except Non Toxic\n","colors.append('#DDF2D1')  # Green color for Non Toxic\n","\n","# Create the bar plot\n","plt.figure(figsize=(10, 6))\n","ax = sns.barplot(x='Category', y='Count', data=data, palette=colors)\n","\n","# Add titles and labels\n","ax.set_title('Count of Comments by Category', fontsize=16, fontweight='bold')\n","ax.set_xlabel('Category', fontsize=14)\n","ax.set_ylabel('Count', fontsize=14)\n","ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n","\n","# Show the plot\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ikde940veIAm"},"outputs":[],"source":["# Load the mask image\n","comment_mask = np.array(Image.open('/content/comment.png'))\n","\n","# Function to generate word cloud\n","def generate_wordcloud(text, title, ax, colormap):\n","    wc = WordCloud(\n","        background_color='white',\n","        max_words=200,\n","        mask=comment_mask,\n","        colormap=colormap,\n","        contour_color='black',\n","        contour_width=1\n","    )\n","    wc.generate(text)\n","    ax.imshow(wc, interpolation='bilinear')\n","    ax.set_title(title, fontdict={'size': 22, 'verticalalignment': 'bottom'})\n","    ax.axis('off')\n","\n","# Create subplots\n","fig, axes = plt.subplots(3, 2, figsize=(24, 15))\n","axes = axes.flatten()  # Flatten the 2D array of axes for easy iteration\n","\n","# Define the categories and their colormaps\n","categories = ['toxic', 'severe_toxic', 'obscene', 'insult', 'identity_hate', 'non_toxic']\n","dataframes = [toxic_df, severe_toxic_df, obscene_toxic_df, insult_df, identity_hate_df, non_toxic_df]\n","colormaps = {\n","    'toxic': 'plasma',  # Purple to red\n","    'severe_toxic': 'plasma',\n","    'obscene': 'plasma',\n","    'insult': 'plasma',\n","    'identity_hate': 'plasma',\n","    'non_toxic': 'summer'  # Green to yellow\n","}\n","\n","# Generate word clouds for each category and add to subplots\n","for i, (category, df) in enumerate(zip(categories, dataframes)):\n","    text = ' '.join(df['comment_text'])\n","    colormap = colormaps.get(category, 'viridis')  # Default to 'viridis' if category not found\n","    generate_wordcloud(text, f'Top words for {category.replace(\"_\", \" \").capitalize()}', axes[i], colormap)\n","\n","# Adjust layout and show plot\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"H3YxPBgIgJ79"},"source":["# ***Data Preparartion***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"20up1JfLeIDB"},"outputs":[],"source":["df = train.copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"STB1P6-VeIHt"},"outputs":[],"source":["df['target'] = (df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) \u003e 0 ).astype(int) #.astype(int): Converts the Boolean result (True or False) to integers (1 or 0).\n","df = df[['comment_text', 'target']]\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ncYpCB1SgJyP"},"outputs":[],"source":["# Calculating the percentage of each class\n","percentage = df['target'].value_counts(normalize=True) * 100\n","\n","# Define colors for the bars\n","colors = ['#DDF2D1' if x == 0 else 'red' for x in percentage.index]\n","\n","# Plotting the percentage of each class\n","plt.figure(figsize=(8, 5))\n","ax = sns.barplot(x=percentage.index, y=percentage, palette=colors)\n","plt.title('Percentage of Toxic and Non Toxic')\n","plt.xlabel('Toxic')\n","plt.ylabel('Percentage (%)')\n","plt.xticks(ticks=[0, 1], labels=['Non Toxic', 'Toxic'])\n","plt.yticks(ticks=range(0, 80, 10))\n","\n","# Displaying the percentage on the bars\n","for i, p in enumerate(percentage):\n","    ax.text(i, p + 0.5, f'{p:.2f}%', ha='center', va='bottom')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"hCwp2Y0LhtkF"},"source":["==\u003e the data is imbalanced"]},{"cell_type":"markdown","metadata":{"id":"s-dDW1SYh0nD"},"source":["# ***Train-Test Split***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S2wjKw2HgJrQ"},"outputs":[],"source":["# Feature-target split\n","X = df['comment_text']\n","y = df['target']\n","\n","# Train-test split (from complete data)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","data_train = pd.concat([X_train, y_train], axis=1)\n","\n","# Validation-test split (from test data)\n","X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=121)\n","data_val, data_test = pd.concat([X_val, y_val], axis=1), pd.concat([X_test, y_test], axis=1)\n","\n","# Comparison of sizes of training set, validation set, and test set\n","values = np.array([len(data_train), len(data_val), len(data_test)])\n","labels = ['Training Set', 'Validation Set', 'Test Set']\n","\n","# Define shades of red\n","colors = ['#ff3333','#ff9999', '#ff6666']\n","\n","# Create the pie chart\n","fig, ax = plt.subplots(figsize=(10, 6))\n","ax.pie(values, labels=labels, autopct='%1.1f%%', startangle=90, pctdistance=0.85, colors=colors)\n","\n","# Draw a white circle at the center to create a donut chart\n","centre_circle = plt.Circle((0, 0), 0.70, fc='white')\n","fig.gca().add_artist(centre_circle)\n","\n","# Equal aspect ratio ensures that pie is drawn as a circle\n","ax.axis('equal')\n","\n","# Add a title\n","plt.title('Comparison of Sizes of Training Set, Validation Set, and Test Set', y=1.05)\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"knEGW794gJod"},"outputs":[],"source":["# RegexpTokenizer\n","regexp = RegexpTokenizer(\"[\\w']+\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nTgtJtojieO-"},"outputs":[],"source":["# Converting to lowercase\n","def convert_to_lowercase(text):\n","    return text.lower()\n","\n","text = \"This is a FUNCTION that CoNvErTs a Text to lowercase\"\n","print(\"Input: {}\".format(text))\n","print(\"Output: {}\".format(convert_to_lowercase(text)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0dvgJTENieRC"},"outputs":[],"source":["# Removing whitespaces\n","def remove_whitespace(text):\n","    return text.strip()\n","\n","text = \" \\t This is a string \\t \"\n","print(\"Input: {}\".format(text))\n","print(\"Output: {}\".format(remove_whitespace(text)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aVy2jddoieUy"},"outputs":[],"source":["# Removing punctuations\n","def remove_punctuation(text):\n","    punct_str = string.punctuation\n","    punct_str = punct_str.replace(\"'\", \"\") # discarding apostrophe from the string to keep the contractions intact\n","    return text.translate(str.maketrans(\"\", \"\", punct_str))\n","\n","text = \"Here's [an] example? {of} \u0026a string. with.? punctuations!!!!\"\n","print(\"Input: {}\".format(text))\n","print(\"Output: {}\".format(remove_punctuation(text)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K1KrTteQieWy"},"outputs":[],"source":["# Removing HTML tags\n","def remove_html(text):\n","    html = re.compile(r'\u003c.*?\u003e')\n","    return html.sub(r'', text)\n","\n","text = ' \u003ca href = \"https://www.kaggle.com/code/vishalnaik/intensity-classification\"\u003e Intensity classification \u003c/a\u003e'\n","print(\"Input: {}\".format(text))\n","print(\"Output: {}\".format(remove_html(text)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2n8MhfdAieZE"},"outputs":[],"source":["\"\"\"\n","üòä - Smiling Face with Smiling Eyes\n","üòÇ - Face with Tears of Joy\n","üòç - Heart Eyes\n","üòí - Unamused Face\n","üò¢ - Crying Face\n","üòé - Smiling Face with Sunglasses\n","üôÅ - Slightly Frowning Face\n","üëç - Thumbs Up\n","üëè - Clapping Hands\n","üôè - Folded Hands\n","‚ù§Ô∏è - Red Heart\n","ü•∫ - Pleading Face\n","ü§î - Thinking Face\n","ü•≥ - Partying Face\n","üòî - Pensive Face\n","üò† - Angry Face\n","ü§ó - Hugging Face\n","üòÖ - Smiling Face with Sweat\n","üò° - Pouting Face\n","ü§© - Star-Struck\n","üôå - Raising Hands\n","üî• - Fire\n","üíî - Broken Heart\n","üéâ - Party Popper\n","üí™ - Flexed Biceps\n","\n","\"\"\"\n","\n","# Define a mapping from emoji names to simplified descriptions\n","emoji_mapping = {\n","    'smiling_face_with_smiling_eyes': 'mocking',\n","    'face_with_tears_of_joy': 'sarcastic laugh',\n","    'heart_eyes': 'obsession',\n","    'crying_face': 'whining',\n","    'angry_face': 'rage',\n","    'enraged_face': 'fury',\n","    'thumbs_up': 'sarcastic approval',\n","    'clapping_hands': 'mocking applause',\n","    'red_heart': 'fake sympathy',\n","    'face_with_rolling_eyes': 'disdain',\n","    'face_with_symbols_on_mouth': 'cursing',\n","    'middle_finger': 'fuck off',\n","    # Add more mappings as needed\n","}\n","\n","# Function to convert emojis to simplified text descriptions\n","def convert_emoji_to_text(text):\n","    # Convert emojis to detailed descriptions\n","    detailed_text = emoji.demojize(text, delimiters=(\":\", \":\"))\n","\n","    # Replace detailed descriptions with simplified ones\n","    for detailed, simplified in emoji_mapping.items():\n","        detailed_text = detailed_text.replace(f':{detailed}:', simplified + ' ')\n","\n","    return detailed_text\n","\n","text = \"This innovative hd printing technique results in durable and spectacular looking prints üòäüòÇüò°üñï‚ù§Ô∏è\"\n","print(\"Input: {}\".format(text))\n","print(\"Output: {}\".format(convert_emoji_to_text(text)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-zHkwYoNieb_"},"outputs":[],"source":["# Removing other unicode characters\n","def remove_http(text):\n","    http = \"https?://\\S+|www\\.\\S+\" # matching strings beginning with http (but not just \"http\")\n","    pattern = r\"({})\".format(http) # creating pattern\n","    return re.sub(pattern, \"\", text)\n","\n","text = \"It's a function that removes links starting with http: or https such as https://en.wikipedia.org/wiki/Unicode_symbols\"\n","print(\"Input: {}\".format(text))\n","print(\"Output: {}\".format(remove_http(text)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CYh6uiv-ieeZ"},"outputs":[],"source":["acronyms_dict = {\n","    \"aka\": \"also known as\",\n","    \"asap\": \"as soon as possible\",\n","    \"brb\": \"be right back\",\n","    \"btw\": \"by the way\",\n","    \"dob\": \"date of birth\",\n","    \"faq\": \"frequently asked questions\",\n","    \"fyi\": \"for your information\",\n","    \"idk\": \"i don't know\",\n","    \"idc\": \"i don't care\",\n","    \"iirc\": \"if i recall correctly\",\n","    \"imo\": \"in my opinion\",\n","    \"irl\": \"in real life\",\n","    \"lmk\": \"let me know\",\n","    \"lol\": \"laugh out loud\",\n","    \"ngl\": \"not gonna lie\",\n","    \"noyb\": \"none of your business\",\n","    \"nvm\": \"never mind\",\n","    \"ofc\": \"of course\",\n","    \"omg\": \"oh my god\",\n","    \"pfa\": \"please find attached\",\n","    \"rofl\": \"rolling on the floor laughing\",\n","    \"stfu\": \"shut the fuck up\",\n","    \"tba\": \"to be announced\",\n","    \"tbc\": \"to be continued\",\n","    \"tbd\": \"to be determined\",\n","    \"tbh\": \"to be honest\",\n","    \"ttyl\": \"talk to you later\",\n","    \"wtf\": \"what the fuck\",\n","    \"wth\": \"what the heck\"\n","}\n","print(\"Example: Original form of the acronym 'fyi' is '{}'\".format(acronyms_dict['fyi']))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mkeugh1Tieg7"},"outputs":[],"source":["# List of acronyms\n","acronyms_list = list(acronyms_dict.keys())\n","\n","# Function to convert contractions in a text\n","def convert_acronyms(text):\n","    words = []\n","    for word in regexp.tokenize(text):\n","        if word in acronyms_list:\n","            words = words + acronyms_dict[word].split()\n","        else:\n","            words = words + word.split()\n","\n","    text_converted = \" \".join(words)\n","    return text_converted\n","\n","text = \"btw you've to fill in the details including dob\"\n","print(\"Input: {}\".format(text))\n","print(\"Output: {}\".format(convert_acronyms(text)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vu6q2cL0gJk0"},"outputs":[],"source":["contractions_dict = {\n","    \"'aight\": \"alright\",\n","    \"ain't\": \"are not\",\n","    \"amn't\": \"am not\",\n","    \"arencha\": \"are not you\",\n","    \"aren't\": \"are not\",\n","    \"'bout\": \"about\",\n","    \"can't\": \"cannot\",\n","    \"cap'n\": \"captain\",\n","    \"'cause\": \"because\",\n","    \"'cept\": \"except\",\n","    \"could've\": \"could have\",\n","    \"couldn't\": \"could not\",\n","    \"couldn't've\": \"could not have\",\n","    \"dammit\": \"damn it\",\n","    \"daren't\": \"dare not\",\n","    \"daresn't\": \"dare not\",\n","    \"dasn't\": \"dare not\",\n","    \"didn't\": \"did not\",\n","    \"doesn't\": \"does not\",\n","    \"doin'\": \"doing\",\n","    \"don't\": \"do not\",\n","    \"dunno\": \"do not know\",\n","    \"d'ye\": \"do you\",\n","    \"e'en\": \"even\",\n","    \"e'er\": \"ever\",\n","    \"'em\": \"them\",\n","    \"everybody's\": \"everybody is\",\n","    \"everyone's\": \"everyone is\",\n","    \"fo'c'sle\": \"forecastle\",\n","    \"finna\": \"fixing to\",\n","    \"'gainst\": \"against\",\n","    \"g'day\": \"good day\",\n","    \"gimme\": \"give me\",\n","    \"giv'n\": \"given\",\n","    \"gonna\": \"going to\",\n","    \"gon't\": \"go not\",\n","    \"gotcha\": \"got you\",\n","    \"gotta\": \"got to\",\n","    \"gtg\": \"got to go\",\n","    \"hadn't\": \"had not\",\n","    \"had've\": \"had have\",\n","    \"hasn't\": \"has not\",\n","    \"haven't\": \"have not\",\n","    \"he'd\": \"he had\",\n","    \"he'll\": \"he shall\",\n","    \"helluva\": \"hell of a\",\n","    \"he's\": \"he is\",\n","    \"here's\": \"here is\",\n","    \"he've\": \"he have\",\n","    \"how'd\": \"how would\",\n","    \"howdy\": \"how do you do\",\n","    \"how'll\": \"how will\",\n","    \"how're\": \"how are\",\n","    \"how's\": \"how is\",\n","    \"i'd\": \"i would\",\n","    \"i'd've\": \"i would have\",\n","    \"i'll\": \"i shall\",\n","    \"i'm\": \"i am\",\n","    \"imma\": \"i am about to\",\n","    \"i'm'a\": \"i am about to\",\n","    \"i'm'o\": \"i am going to\",\n","    \"innit\": \"is it not\",\n","    \"ion\": \"i do not\",\n","    \"i've\": \"i have\",\n","    \"i'd\": \"i had\",\n","    \"i'd've\": \"i would have\",\n","    \"i'll\": \"i shall\",\n","    \"i'm\": \"i am\",\n","    \"i'm'a\": \"i am about to\",\n","    \"i'm'o\": \"i am going to\",\n","    \"innit\": \"is it not\",\n","    \"i've\": \"i have\",\n","    \"isn't\": \"is not\",\n","    \"it'd\": \"it would\",\n","    \"it'll\": \"it shall\",\n","    \"it's\": \"it is\",\n","    \"iunno\": \"i do not know\",\n","    \"kinda\": \"kind of\",\n","    \"let's\": \"let us\",\n","    \"li'l\": \"little\",\n","    \"ma'am\": \"madam\",\n","    \"mayn't\": \"may not\",\n","    \"may've\": \"may have\",\n","    \"methinks\": \"me thinks\",\n","    \"mightn't\": \"might not\",\n","    \"might've\": \"might have\",\n","    \"mustn't\": \"must not\",\n","    \"mustn't've\": \"must not have\",\n","    \"must've\": \"must have\",\n","    \"'neath\": \"beneath\",\n","    \"needn't\": \"need not\",\n","    \"nal\": \"and all\",\n","    \"ne'er\": \"never\",\n","    \"o'clock\": \"of the clock\",\n","    \"o'er\": \"over\",\n","    \"ol'\": \"old\",\n","    \"oughtn't\": \"ought not\",\n","    \"'round\": \"around\",\n","    \"'s\": \"is\",\n","    \"shalln't\": \"shall not\",\n","    \"shan't\": \"shall not\",\n","    \"she'd\": \"she had\",\n","    \"she'll\": \"she shall\",\n","    \"she's\": \"she is\",\n","    \"should've\": \"should have\",\n","    \"shouldn't\": \"should not\",\n","    \"shouldn't've\": \"should not have\",\n","    \"somebody's\": \"somebody is\",\n","    \"someone's\": \"someone is\",\n","    \"something's\": \"something is\",\n","    \"so're\": \"so are\",\n","    \"so's\": \"so is\",\n","    \"so've\": \"so have\",\n","    \"that'll\": \"that shall\",\n","    \"that're\": \"that are\",\n","    \"that's\": \"that is\",\n","    \"that'd\": \"that would\",\n","    \"there'd\": \"there had\",\n","    \"there'll\": \"there shall\",\n","    \"there're\": \"there are\",\n","    \"there's\": \"there is\",\n","    \"these're\": \"these are\",\n","    \"these've\": \"these have\",\n","    \"they'd\": \"they had\",\n","    \"they'll\": \"they shall\",\n","    \"they're\": \"they are\",\n","    \"they've\": \"they have\",\n","    \"this's\": \"this is\",\n","    \"those're\": \"those are\",\n","    \"those've\": \"those have\",\n","    \"'thout\": \"without\",\n","    \"'til\": \"until\",\n","    \"'tis\": \"it is\",\n","    \"to've\": \"to have\",\n","    \"'twas\": \"it was\",\n","    \"'tween\": \"between\",\n","    \"'twhere\": \"it were\",\n","    \"wanna\": \"want to\",\n","    \"wasn't\": \"was not\",\n","    \"we'd\": \"we had\",\n","    \"we'd've\": \"we would have\",\n","    \"we'll\": \"we shall\",\n","    \"we're\": \"we are\",\n","    \"we've\": \"we have\",\n","    \"weren't\": \"were not\",\n","    \"whatcha\": \"what are you\",\n","    \"what'd\": \"what did\",\n","    \"what'll\": \"what shall\",\n","    \"what're\": \"what are\",\n","    \"what's\": \"what is\",\n","    \"what've\": \"what have\",\n","    \"when's\": \"when is\",\n","    \"where'd\": \"where did\",\n","    \"where'll\": \"where shall\",\n","    \"where're\": \"where are\",\n","    \"where's\": \"where is\",\n","    \"where've\": \"where have\",\n","    \"which'd\": \"which had\",\n","    \"which'll\": \"which shall\",\n","    \"which're\": \"which are\",\n","    \"which's\": \"which is\",\n","    \"which've\": \"which have\",\n","    \"who'd\": \"who would\",\n","    \"who'd've\": \"who would have\",\n","    \"who'll\": \"who shall\",\n","    \"who're\": \"who are\",\n","    \"who's\": \"who is\",\n","    \"who've\": \"who have\",\n","    \"why'd\": \"why did\",\n","    \"why're\": \"why are\",\n","    \"why's\": \"why is\",\n","    \"willn't\": \"will not\",\n","    \"won't\": \"will not\",\n","    \"wonnot\": \"will not\",\n","    \"would've\": \"would have\",\n","    \"wouldn't\": \"would not\",\n","    \"wouldn't've\": \"would not have\",\n","    \"y'all\": \"you all\",\n","    \"y'all'd've\": \"you all would have\",\n","    \"y'all'd'n't've\": \"you all would not have\",\n","    \"y'all're\": \"you all are\",\n","    \"y'all'ren't\": \"you all are not\",\n","    \"y'at\": \"you at\",\n","    \"yes'm\": \"yes madam\",\n","    \"yessir\": \"yes sir\",\n","    \"you'd\": \"you had\",\n","    \"you'll\": \"you shall\",\n","    \"you're\": \"you are\",\n","    \"you've\": \"you have\",\n","    \"aight\": \"alright\",\n","    \"aint\": \"are not\",\n","    \"amnt\": \"am not\",\n","    \"arent\": \"are not\",\n","    \"cant\": \"cannot\",\n","    \"cause\": \"because\",\n","    \"couldve\": \"could have\",\n","    \"couldnt\": \"could not\",\n","    \"couldntve\": \"could not have\",\n","    \"darent\": \"dare not\",\n","    \"daresnt\": \"dare not\",\n","    \"dasnt\": \"dare not\",\n","    \"didnt\": \"did not\",\n","    \"doesnt\": \"does not\",\n","    \"doin\": \"doing\",\n","    \"dont\": \"do not\",\n","    \"eer\": \"ever\",\n","    \"everybodys\": \"everybody is\",\n","    \"everyones\": \"everyone is\",\n","    \"finna\": \"fixing to\",\n","    \"gday\": \"good day\",\n","    \"givn\": \"given\",\n","    \"gont\": \"go not\",\n","    \"hadnt\": \"had not\",\n","    \"hadve\": \"had have\",\n","    \"hasnt\": \"has not\",\n","    \"havent\": \"have not\",\n","    \"hed\": \"he had\",\n","    \"hell\": \"he shall\",\n","    \"hes\": \"he is\",\n","    \"heve\": \"he have\",\n","    \"howd\": \"how did\",\n","    \"howdy\": \"how do you do\",\n","    \"howll\": \"how will\",\n","    \"howre\": \"how are\",\n","    \"hows\": \"how is\",\n","    \"idve\": \"i would have\",\n","    \"ill\": \"i shall\",\n","    \"im\": \"i am\",\n","    \"ima\": \"i am about to\",\n","    \"imo\": \"i am going to\",\n","    \"innit\": \"is it not\",\n","    \"ive\": \"i have\",\n","    \"isnt\": \"is not\",\n","    \"itd\": \"it would\",\n","    \"itll\": \"it shall\",\n","    \"its\": \"it is\",\n","    \"lets\": \"let us\",\n","    \"lil\": \"little\",\n","    \"maam\": \"madam\",\n","    \"maynt\": \"may not\",\n","    \"mayve\": \"may have\",\n","    \"methinks\": \"me thinks\",\n","    \"mightnt\": \"might not\",\n","    \"mightve\": \"might have\",\n","    \"mustnt\": \"must not\",\n","    \"mustntve\": \"must not have\",\n","    \"mustve\": \"must have\",\n","    \"neednt\": \"need not\",\n","    \"neer\": \"never\",\n","    \"oclock\": \"of the clock\",\n","    \"oer\": \"over\",\n","    \"ol\": \"old\",\n","    \"oughtnt\": \"ought not\",\n","    \"shallnt\": \"shall not\",\n","    \"shant\": \"shall not\",\n","    \"shed\": \"she had\",\n","    \"shell\": \"she shall\",\n","    \"shes\": \"she is\",\n","    \"shouldve\": \"should have\",\n","    \"shouldnt\": \"should not\",\n","    \"shouldntve\": \"should not have\",\n","    \"somebodys\": \"somebody is\",\n","    \"someones\": \"someone is\",\n","    \"somethings\": \"something is\",\n","    \"thatll\": \"that shall\",\n","    \"thatre\": \"that are\",\n","    \"thatd\": \"that would\",\n","    \"thered\": \"there had\",\n","    \"therell\": \"there shall\",\n","    \"therere\": \"there are\",\n","    \"theres\": \"there is\",\n","    \"thesere\": \"these are\",\n","    \"theseve\": \"these have\",\n","    \"theyd\": \"they had\",\n","    \"theyll\": \"they shall\",\n","    \"theyre\": \"they are\",\n","    \"theyve\": \"they have\",\n","    \"thiss\": \"this is\",\n","    \"thosere\": \"those are\",\n","    \"thoseve\": \"those have\",\n","    \"tis\": \"it is\",\n","    \"tove\": \"to have\",\n","    \"twas\": \"it was\",\n","    \"wanna\": \"want to\",\n","    \"wasnt\": \"was not\",\n","    \"wed\": \"we had\",\n","    \"wedve\": \"we would have\",\n","    \"were\": \"we are\",\n","    \"weve\": \"we have\",\n","    \"werent\": \"were not\",\n","    \"whatd\": \"what did\",\n","    \"whatll\": \"what shall\",\n","    \"whatre\": \"what are\",\n","    \"whats\": \"what is\",\n","    \"whatve\": \"what have\",\n","    \"whens\": \"when is\",\n","    \"whered\": \"where did\",\n","    \"wherell\": \"where shall\",\n","    \"wherere\": \"where are\",\n","    \"wheres\": \"where is\",\n","    \"whereve\": \"where have\",\n","    \"whichd\": \"which had\",\n","    \"whichll\": \"which shall\",\n","    \"whichre\": \"which are\",\n","    \"whichs\": \"which is\",\n","    \"whichve\": \"which have\",\n","    \"whod\": \"who would\",\n","    \"whodve\": \"who would have\",\n","    \"wholl\": \"who shall\",\n","    \"whore\": \"who are\",\n","    \"whos\": \"who is\",\n","    \"whove\": \"who have\",\n","    \"whyd\": \"why did\",\n","    \"whyre\": \"why are\",\n","    \"whys\": \"why is\",\n","    \"wont\": \"will not\",\n","    \"wouldve\": \"would have\",\n","    \"wouldnt\": \"would not\",\n","    \"wouldntve\": \"would not have\",\n","    \"yall\": \"you all\",\n","    \"yalldve\": \"you all would have\",\n","    \"yallre\": \"you all are\",\n","    \"youd\": \"you had\",\n","    \"youll\": \"you shall\",\n","    \"youre\": \"you are\",\n","    \"youve\": \"you have\",\n","    \"'re\": \"are\",\n","    \"that's\": \"that is\",\n","    \"thats\": \"that is\"\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hjKiHg6FeIKm"},"outputs":[],"source":["# List of contractions\n","contractions_list = list(contractions_dict.keys())\n","\n","# Function to convert contractions in a text\n","def convert_contractions(text):\n","    words = []\n","    for word in regexp.tokenize(text):\n","        if word in contractions_list:\n","            words = words + contractions_dict[word].split()\n","        else:\n","            words = words + word.split()\n","\n","    text_converted = \" \".join(words)\n","    return text_converted\n","\n","text = \"it's function that make can't removes i'm\"\n","\n","print(\"Input: {}\".format(text))\n","print(\"Output: {}\".format(convert_contractions(text)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0yc-7fUSeIMp"},"outputs":[],"source":["abbreviations = {\n","    \"$\" : \"dollar\",\n","    \"‚Ç¨\" : \" euro \",\n","    \"4ao\" : \"for adults only\",\n","    \"a.m\" : \"before midday\",\n","    \"a3\" : \"anytime anywhere anyplace\",\n","    \"aamof\" : \"as a matter of fact\",\n","    \"acct\" : \"account\",\n","    \"adih\" : \"another day in hell\",\n","    \"afaic\" : \"as far as i am concerned\",\n","    \"afaict\" : \"as far as i can tell\",\n","    \"afaik\" : \"as far as i know\",\n","    \"afair\" : \"as far as i remember\",\n","    \"afk\" : \"away from keyboard\",\n","    \"app\" : \"application\",\n","    \"approx\" : \"approximately\",\n","    \"apps\" : \"applications\",\n","    \"asap\" : \"as soon as possible\",\n","    \"asl\" : \"age, sex, location\",\n","    \"atk\" : \"at the keyboard\",\n","    \"ave.\" : \"avenue\",\n","    \"aymm\" : \"are you my mother\",\n","    \"ayor\" : \"at your own risk\",\n","    \"b\u0026b\" : \"bed and breakfast\",\n","    \"b+b\" : \"bed and breakfast\",\n","    \"b.c\" : \"before christ\",\n","    \"b2b\" : \"business to business\",\n","    \"b2c\" : \"business to customer\",\n","    \"b4\" : \"before\",\n","    \"b4n\" : \"bye for now\",\n","    \"b@u\" : \"back at you\",\n","    \"bae\" : \"before anyone else\",\n","    \"bak\" : \"back at keyboard\",\n","    \"bbbg\" : \"bye bye be good\",\n","    \"bbc\" : \"british broadcasting corporation\",\n","    \"bbias\" : \"be back in a second\",\n","    \"bbl\" : \"be back later\",\n","    \"bbs\" : \"be back soon\",\n","    \"be4\" : \"before\",\n","    \"bfn\" : \"bye for now\",\n","    \"blvd\" : \"boulevard\",\n","    \"bout\" : \"about\",\n","    \"brb\" : \"be right back\",\n","    \"bros\" : \"brothers\",\n","    \"brt\" : \"be right there\",\n","    \"bsaaw\" : \"big smile and a wink\",\n","    \"btw\" : \"by the way\",\n","    \"bwl\" : \"bursting with laughter\",\n","    \"c/o\" : \"care of\",\n","    \"cet\" : \"central european time\",\n","    \"cf\" : \"compare\",\n","    \"cia\" : \"central intelligence agency\",\n","    \"csl\" : \"can not stop laughing\",\n","    \"cu\" : \"see you\",\n","    \"cul8r\" : \"see you later\",\n","    \"cv\" : \"curriculum vitae\",\n","    \"cwot\" : \"complete waste of time\",\n","    \"cya\" : \"see you\",\n","    \"cyt\" : \"see you tomorrow\",\n","    \"dae\" : \"does anyone else\",\n","    \"dbmib\" : \"do not bother me i am busy\",\n","    \"diy\" : \"do it yourself\",\n","    \"dm\" : \"direct message\",\n","    \"dwh\" : \"during work hours\",\n","    \"e123\" : \"easy as one two three\",\n","    \"eet\" : \"eastern european time\",\n","    \"eg\" : \"example\",\n","    \"embm\" : \"early morning business meeting\",\n","    \"encl\" : \"enclosed\",\n","    \"encl.\" : \"enclosed\",\n","    \"etc\" : \"and so on\",\n","    \"faq\" : \"frequently asked questions\",\n","    \"fawc\" : \"for anyone who cares\",\n","    \"fb\" : \"facebook\",\n","    \"fc\" : \"fingers crossed\",\n","    \"fig\" : \"figure\",\n","    \"fimh\" : \"forever in my heart\",\n","    \"ft.\" : \"feet\",\n","    \"ft\" : \"featuring\",\n","    \"ftl\" : \"for the loss\",\n","    \"ftw\" : \"for the win\",\n","    \"fwiw\" : \"for what it is worth\",\n","    \"fyi\" : \"for your information\",\n","    \"g9\" : \"genius\",\n","    \"gahoy\" : \"get a hold of yourself\",\n","    \"gal\" : \"get a life\",\n","    \"gcse\" : \"general certificate of secondary education\",\n","    \"gfn\" : \"gone for now\",\n","    \"gg\" : \"good game\",\n","    \"gl\" : \"good luck\",\n","    \"glhf\" : \"good luck have fun\",\n","    \"gmt\" : \"greenwich mean time\",\n","    \"gmta\" : \"great minds think alike\",\n","    \"gn\" : \"good night\",\n","    \"g.o.a.t\" : \"greatest of all time\",\n","    \"goat\" : \"greatest of all time\",\n","    \"goi\" : \"get over it\",\n","    \"gps\" : \"global positioning system\",\n","    \"gr8\" : \"great\",\n","    \"gratz\" : \"congratulations\",\n","    \"gyal\" : \"girl\",\n","    \"h\u0026c\" : \"hot and cold\",\n","    \"hp\" : \"horsepower\",\n","    \"hr\" : \"hour\",\n","    \"hrh\" : \"his royal highness\",\n","    \"ht\" : \"height\",\n","    \"ibrb\" : \"i will be right back\",\n","    \"ic\" : \"i see\",\n","    \"icq\" : \"i seek you\",\n","    \"icymi\" : \"in case you missed it\",\n","    \"idc\" : \"i do not care\",\n","    \"idgadf\" : \"i do not give a damn fuck\",\n","    \"idgaf\" : \"i do not give a fuck\",\n","    \"idk\" : \"i do not know\",\n","    \"ie\" : \"that is\",\n","    \"i.e\" : \"that is\",\n","    \"ifyp\" : \"i feel your pain\",\n","    \"IG\" : \"instagram\",\n","    \"iirc\" : \"if i remember correctly\",\n","    \"ilu\" : \"i love you\",\n","    \"ily\" : \"i love you\",\n","    \"imho\" : \"in my humble opinion\",\n","    \"imo\" : \"in my opinion\",\n","    \"imu\" : \"i miss you\",\n","    \"iow\" : \"in other words\",\n","    \"irl\" : \"in real life\",\n","    \"j4f\" : \"just for fun\",\n","    \"jic\" : \"just in case\",\n","    \"jk\" : \"just kidding\",\n","    \"jsyk\" : \"just so you know\",\n","    \"l8r\" : \"later\",\n","    \"lb\" : \"pound\",\n","    \"lbs\" : \"pounds\",\n","    \"ldr\" : \"long distance relationship\",\n","    \"lmao\" : \"laugh my ass off\",\n","    \"lmfao\" : \"laugh my fucking ass off\",\n","    \"lol\" : \"laughing out loud\",\n","    \"ltd\" : \"limited\",\n","    \"ltns\" : \"long time no see\",\n","    \"m8\" : \"mate\",\n","    \"mf\" : \"motherfucker\",\n","    \"mfs\" : \"motherfuckers\",\n","    \"mfw\" : \"my face when\",\n","    \"mofo\" : \"motherfucker\",\n","    \"mph\" : \"miles per hour\",\n","    \"mr\" : \"mister\",\n","    \"mrw\" : \"my reaction when\",\n","    \"ms\" : \"miss\",\n","    \"mte\" : \"my thoughts exactly\",\n","    \"nagi\" : \"not a good idea\",\n","    \"nbc\" : \"national broadcasting company\",\n","    \"nbd\" : \"not big deal\",\n","    \"nfs\" : \"not for sale\",\n","    \"ngl\" : \"not going to lie\",\n","    \"nhs\" : \"national health service\",\n","    \"nrn\" : \"no reply necessary\",\n","    \"nsfl\" : \"not safe for life\",\n","    \"nsfw\" : \"not safe for work\",\n","    \"nth\" : \"nice to have\",\n","    \"nvr\" : \"never\",\n","    \"nyc\" : \"new york city\",\n","    \"oc\" : \"original content\",\n","    \"og\" : \"original\",\n","    \"ohp\" : \"overhead projector\",\n","    \"oic\" : \"oh i see\",\n","    \"omdb\" : \"over my dead body\",\n","    \"omg\" : \"oh my god\",\n","    \"omw\" : \"on my way\",\n","    \"p.a\" : \"per annum\",\n","    \"p.m\" : \"after midday\",\n","    \"pm\" : \"prime minister\",\n","    \"poc\" : \"people of color\",\n","    \"pov\" : \"point of view\",\n","    \"pp\" : \"pages\",\n","    \"ppl\" : \"people\",\n","    \"prw\" : \"parents are watching\",\n","    \"ps\" : \"postscript\",\n","    \"pt\" : \"point\",\n","    \"ptb\" : \"please text back\",\n","    \"pto\" : \"please turn over\",\n","    \"qpsa\" : \"what happens\", #\"que pasa\",\n","    \"ratchet\" : \"rude\",\n","    \"rbtl\" : \"read between the lines\",\n","    \"rlrt\" : \"real life retweet\",\n","    \"rofl\" : \"rolling on the floor laughing\",\n","    \"roflol\" : \"rolling on the floor laughing out loud\",\n","    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n","    \"rt\" : \"retweet\",\n","    \"ruok\" : \"are you ok\",\n","    \"sfw\" : \"safe for work\",\n","    \"sk8\" : \"skate\",\n","    \"smh\" : \"shake my head\",\n","    \"sq\" : \"square\",\n","    \"srsly\" : \"seriously\",\n","    \"ssdd\" : \"same stuff different day\",\n","    \"tbh\" : \"to be honest\",\n","    \"tbs\" : \"tablespooful\",\n","    \"tbsp\" : \"tablespooful\",\n","    \"tfw\" : \"that feeling when\",\n","    \"thks\" : \"thank you\",\n","    \"tho\" : \"though\",\n","    \"thx\" : \"thank you\",\n","    \"tia\" : \"thanks in advance\",\n","    \"til\" : \"today i learned\",\n","    \"tl;dr\" : \"too long i did not read\",\n","    \"tldr\" : \"too long i did not read\",\n","    \"tmb\" : \"tweet me back\",\n","    \"tntl\" : \"trying not to laugh\",\n","    \"ttyl\" : \"talk to you later\",\n","    \"u\" : \"you\",\n","    \"u2\" : \"you too\",\n","    \"u4e\" : \"yours for ever\",\n","    \"utc\" : \"coordinated universal time\",\n","    \"w/\" : \"with\",\n","    \"w/o\" : \"without\",\n","    \"w8\" : \"wait\",\n","    \"wassup\" : \"what is up\",\n","    \"wb\" : \"welcome back\",\n","    \"wtf\" : \"what the fuck\",\n","    \"wtg\" : \"way to go\",\n","    \"wtpa\" : \"where the party at\",\n","    \"wuf\" : \"where are you from\",\n","    \"wuzup\" : \"what is up\",\n","    \"wywh\" : \"wish you were here\",\n","    \"yd\" : \"yard\",\n","    \"ygtr\" : \"you got that right\",\n","    \"ynk\" : \"you never know\",\n","    \"zzz\" : \"sleeping bored and tired\",\n","    \"lol\" : \"laugh out loud\",\n","    \"lit\": \"exciting\",\n","    \"btw\":\"by the way\",\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-MaHs_6aeIP8"},"outputs":[],"source":["nltk.download('punkt')\n","\n","def convert_abbrev_in_text(text):\n","    tokens = word_tokenize(text)\n","    tokens = [abbreviations[word.lower()] if word.lower() in abbreviations else word for word in tokens]\n","    text = ' '.join(tokens)\n","    return text\n","\n","text = \"w/o lol, that party was so lit last night! BTW, did you see how many people were taking selfies?\"\n","print(\"Input: {}\".format(text))\n","print(\"Output: {}\".format(convert_abbrev_in_text(text)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4yuu4ZYWcZj8"},"outputs":[],"source":["# pyspellchecker\n","spell = SpellChecker()\n","\n","def pyspellchecker(text):\n","    word_list = regexp.tokenize(text)\n","    word_list_corrected = []\n","    for word in word_list:\n","        if word in spell.unknown(word_list):\n","            word_corrected = spell.correction(word)\n","            if word_corrected == None:\n","                word_list_corrected.append(word)\n","            else:\n","                word_list_corrected.append(word_corrected)\n","        else:\n","            word_list_corrected.append(word)\n","    text_corrected = \" \".join(word_list_corrected)\n","    return text_corrected\n","\n","text = \"I'm goinng therre\"\n","print(\"Input: {}\".format(text))\n","print(\"Output: {}\".format(pyspellchecker(text)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wlpfe6cib2Bt"},"outputs":[],"source":["# Stemming\n","stemmer = PorterStemmer()\n","def text_stemmer(text):\n","    text_stem = \" \".join([stemmer.stem(word) for word in regexp.tokenize(text)])\n","    return text_stem\n","\n","text = \"Introducing lemmatization as an improvement over stemming\"\n","print(\"Input: {}\".format(text))\n","print(\"Output: {}\".format(text_stemmer(text)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JhgrNooKkd_M"},"outputs":[],"source":["# Lemmatization\n","\n","spacy_lemmatizer = spacy.load(\"en_core_web_sm\", disable = ['parser', 'ner'])\n","#lemmatizer = WordNetLemmatizer()\n","\n","def text_lemmatizer(text):\n","    text_spacy = \" \".join([token.lemma_ for token in spacy_lemmatizer(text)])\n","    #text_wordnet = \" \".join([lemmatizer.lemmatize(word) for word in word_tokenize(text)]) # regexp.tokenize(text)\n","    return text_spacy\n","    #return text_wordnet\n","\n","text = \"Introducing lemmatization as an improvement over stemming\"\n","print(\"Input: {}\".format(text))\n","print(\"Output: {}\".format(text_lemmatizer(text)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OkxzNIV5ki-M"},"outputs":[],"source":["# Discardment of non-alphabetic words\n","def discard_non_alpha(text):\n","    word_list_non_alpha = [word for word in regexp.tokenize(text) if word.isalpha()]\n","    text_non_alpha = \" \".join(word_list_non_alpha)\n","    return text_non_alpha\n","\n","text = \"It is an ocean of thousands and 1000s of crowd\"\n","print(\"Input: {}\".format(text))\n","print(\"Output: {}\".format(discard_non_alpha(text)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zrSpjh6EkjA3"},"outputs":[],"source":["# Ensure the necessary NLTK data is downloaded\n","nltk.download('averaged_perceptron_tagger')\n","\n","# Create a RegexpTokenizer instance\n","regexp = RegexpTokenizer(r'\\w+')\n","\n","def keep_pos(text):\n","    tokens = regexp.tokenize(text)\n","    tokens_tagged = nltk.pos_tag(tokens)\n","    #keep_tags = ['NN', 'NNS', 'NNP', 'NNPS', 'FW']\n","    keep_tags = ['NN', 'NNS', 'NNP', 'NNPS', 'PRP', 'PRPS','JJ',\n","                 'RB', 'RBR', 'RBS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP','WP$',\n","                 'VBZ', 'WDT', 'WP', 'WPS', 'WRB']\n","\n","    keep_words = [x[0] for x in tokens_tagged if x[1] in keep_tags]\n","    return \" \".join(keep_words)\n","\n","text = \"I‚ÄôM sorry If I Change But You Changed Too\"\n","print(\"Input: {}\".format(text))\n","tokens = regexp.tokenize(text)\n","print(\"Tokens: {}\".format(tokens))\n","tokens_tagged = nltk.pos_tag(tokens)\n","print(\"Tagged Tokens: {}\".format(tokens_tagged))\n","print(\"Output: {}\".format(keep_pos(text)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ks0rbuCDkjDw"},"outputs":[],"source":["nltk.download('stopwords')\n","stops = stopwords.words(\"english\") # stopwords\n","alphabets = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\n","all_stops =  stops + alphabets\n","\n","\n","print(all_stops)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qq9oD5-2kjGN"},"outputs":[],"source":["def remove_stopwords(text):\n","    return \" \".join([word for word in regexp.tokenize(text) if word not in all_stops])\n","\n","text = \"i am sad. i got fail in one subject, wait i will share you on whatsapp\"\n","print(\"Input: {}\".format(text))\n","print(\"Output: {}\".format(remove_stopwords(text)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4NoH8hnXkeCE"},"outputs":[],"source":["#Work Flow Integration\n","def text_normalizer(text):\n","    text = convert_to_lowercase(text)\n","    text = remove_whitespace(text)\n","    text = re.sub('\\n' , '', text) # converting text to one line\n","    text = re.sub(r'\\.com\\b', '', text) # Remove \".com\"\n","    text = remove_http(text)\n","    text = remove_punctuation(text)\n","    text = remove_html(text)\n","    text = convert_emoji_to_text(text)\n","    text = convert_acronyms(text)\n","    text = convert_contractions(text)\n","    text = convert_abbrev_in_text(text)\n","#     text = pyspellchecker(text)\n","    text = text_lemmatizer(text) # text = text_stemmer(text)\n","    text = discard_non_alpha(text)\n","    text = keep_pos(text)\n","    text = convert_to_lowercase(text)\n","    text = remove_stopwords(text)\n","\n","    return text\n","\n","text = \"\"\"'I‚ÄôM sorry üò°üñï‚ù§Ô∏è If I Change But You Changed Too. \u0026 We'll [#* combine all ] {functions \u003cinto 1\u003e SINGLE 1000 FUNCTION  \u0026 apply  on @product #content make\n"," removes  https://en.wikipedia.org/wiki/Text_normalization\"\"\"\n","\n","\n","print(\"Input: {}\".format(text))\n","print(\"Output: {}\".format(text_normalizer(text)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tGWxjOE5keE0"},"outputs":[],"source":["# Implementing text normalization\n","data_train_norm, data_val_norm, data_test_norm = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n","\n","data_train_norm['normalized_comment_text'] = data_train['comment_text'].apply(text_normalizer)\n","data_val_norm['normalized_comment_text'] = data_val['comment_text'].apply(text_normalizer)\n","data_test_norm['normalized_comment_text'] = data_test['comment_text'].apply(text_normalizer)\n","\n","data_train_norm['target'] = data_train['target']\n","data_val_norm['target'] = data_val['target']\n","data_test_norm['target'] = data_test['target']\n","\n","data_train_norm['normalized_comment_text']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rLTs_hqvmCio"},"outputs":[],"source":["from wordcloud import WordCloud\n","\n","# Function to generate word cloud\n","def generate_wordcloud(text, title, ax):\n","    wc = WordCloud(\n","        background_color='white',\n","        max_words=200,\n","        colormap='plasma',\n","        contour_color='black',\n","        contour_width=1\n","    )\n","    wc.generate(text)\n","    ax.imshow(wc, interpolation='bilinear')\n","    ax.set_title(title, fontdict={'size': 22, 'verticalalignment': 'bottom'})\n","    ax.axis('off')\n","\n","# Create subplots\n","fig, axes = plt.subplots(1, 3, figsize=(24, 20))\n","\n","# Define the categories and corresponding datasets\n","categories = ['Training', 'Validation', 'Test']\n","datasets = [data_train_norm, data_val_norm, data_test_norm]\n","\n","# Generate word clouds for each category and add to subplots\n","for i, (category, dataset) in enumerate(zip(categories, datasets)):\n","    text = \" \".join(description for description in dataset['normalized_comment_text'])\n","    generate_wordcloud(text, f'Normalized words for {category.capitalize()}', axes[i])\n","\n","# Adjust layout and show plot\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"31659xZUvIYe"},"source":["*** TF-IDF Vectorization***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bGVvev0PmCk_"},"outputs":[],"source":["# Features and labels\n","X_train_norm, y_train = data_train_norm['normalized_comment_text'].tolist(), data_train_norm['target'].tolist()\n","X_val_norm, y_val = data_val_norm['normalized_comment_text'].tolist(), data_val_norm['target'].tolist()\n","X_test_norm, y_test = data_test_norm['normalized_comment_text'].tolist(), data_test_norm['target'].tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7zBE8eWjmCnj"},"outputs":[],"source":["# TF-IDF vectorization\n","# TfidfVec = TfidfVectorizer()\n","TfidfVec = TfidfVectorizer(ngram_range = (1, 2),max_features=400) # using both unigram \u0026 bi-grams\n","\n","X_train_tfidf = TfidfVec.fit_transform(X_train_norm)\n","X_val_tfidf = TfidfVec.transform(X_val_norm)\n","X_test_tfidf = TfidfVec.transform(X_test_norm)"]},{"cell_type":"markdown","metadata":{"id":"vaJozpB1vYOM"},"source":["***Model Building***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YlLgpdLamCsp"},"outputs":[],"source":["# Classifiers\n","names = [\n","    \"ComplementNB\",\n","    \"Logistic Regression\",\n","    \"KNN Classifier\",\n","    \"Decision Tree\",\n","    \"Linear SVM\",\n","    \"Random Forest\",\n","    \"SGD Classifier\",\n","    \"Ridge Classifier\",\n","]\n","\n","models = [\n","    ComplementNB(),\n","    LogisticRegression(max_iter = 1000,class_weight= 'balanced'),\n","    KNeighborsClassifier(n_neighbors = 149, n_jobs = -1),\n","    DecisionTreeClassifier(class_weight= 'balanced'),\n","    svm.SVC(kernel = 'linear', class_weight= 'balanced'),\n","    RandomForestClassifier(n_estimators = 100, class_weight= 'balanced'),\n","    SGDClassifier(loss = 'hinge',class_weight= 'balanced'),\n","    RidgeClassifier(class_weight= 'balanced'),\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FwETcxWqkeHt"},"outputs":[],"source":["# Function to return summary of baseline models\n","def score(X_train, y_train, X_test, y_test, X_val, y_val, names=names, models=models):\n","    score_df = pd.DataFrame()\n","    score_train, score_test, score_val = [], [], []\n","\n","    for model in models:\n","        model.fit(X_train, y_train)\n","        y_train_pred = model.predict(X_train)\n","        y_test_pred = model.predict(X_test)\n","        y_val_pred = model.predict(X_val)\n","\n","        score_train.append(f1_score(y_train, y_train_pred, average='weighted'))\n","        score_test.append(f1_score(y_test, y_test_pred, average='weighted'))\n","        score_val.append(f1_score(y_val, y_val_pred, average='weighted'))\n","\n","    score_df[\"Classifier\"] = names\n","    score_df[\"Training F1-score\"] = score_train\n","    score_df[\"Test F1-score\"] = score_test\n","    score_df[\"Validation F1-score\"] = score_val\n","\n","    score_df.sort_values(by='Validation F1-score', ascending=False, inplace=True)\n","    return score_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Mu9BBNNvliS"},"outputs":[],"source":["# Summary of baseline models\n","score(X_train_tfidf, y_train,X_test_tfidf, y_test, X_val_tfidf, y_val, names = names, models = models)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xie61JK6voGh"},"outputs":[],"source":["def evaluate_model(model, X_train, y_train, X_test, y_test, X_val=None, y_val=None):\n","    # Evaluate the model on the train data\n","    print(\"\\n\\t  Classification report for training set\")\n","    print(\"-\"*55)\n","    print(classification_report(y_train, model.predict(X_train), target_names=['Non-toxic', 'toxic']))\n","\n","    # Evaluate the model on the test data\n","    print(\"\\n\\t   Classification report for test set\")\n","    print(\"-\"*55)\n","    print(classification_report(y_test, model.predict(X_test), target_names=['Non-toxic', 'toxic']))\n","\n","    if X_val is not None and y_val is not None:\n","        # Evaluate the model on the validation data\n","        print(\"\\n\\t   Classification report for validation set\")\n","        print(\"-\"*55)\n","        print(classification_report(y_val, model.predict(X_val), target_names=['Non-toxic', 'toxic']))\n","\n","    # Generate confusion matrix for test data\n","    cm = confusion_matrix(y_test, model.predict(X_test))\n","\n","    # Define the class labels\n","    class_labels = ['Non-toxic', 'toxic']\n","\n","    # Plot the confusion matrix as a heatmap\n","    plt.figure(figsize=(14, 6))\n","    plt.subplot(1, 2, 1)\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n","                xticklabels=class_labels, yticklabels=class_labels)\n","    plt.title('Confusion Matrix - Test Set')\n","\n","    # Plot ROC curve for each class on the test data\n","    plt.subplot(1, 2, 2)\n","    fpr = {}\n","    tpr = {}\n","    roc_auc = {}\n","    for i in range(len(class_labels)):\n","        fpr[i], tpr[i], _ = roc_curve(y_test, model.predict_proba(X_test)[:, i], pos_label=i)\n","        roc_auc[i] = auc(fpr[i], tpr[i])\n","        plt.plot(fpr[i], tpr[i], lw=2, label=f'{class_labels[i]} (AUC = {roc_auc[i]:0.2f})')\n","    plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('ROC Curve - Test Set')\n","    plt.legend(loc='lower right')\n","    plt.tight_layout()\n","\n","    plt.show()\n","\n","    if X_val is not None and y_val is not None:\n","        # Generate confusion matrix for validation data\n","        cm_val = confusion_matrix(y_val, model.predict(X_val))\n","\n","        # Plot the confusion matrix as a heatmap\n","        plt.figure(figsize=(14, 6))\n","        plt.subplot(1, 2, 1)\n","        sns.heatmap(cm_val, annot=True, fmt='d', cmap='Blues', cbar=False,\n","                    xticklabels=class_labels, yticklabels=class_labels)\n","        plt.title('Confusion Matrix - Validation Set')\n","\n","        # Plot ROC curve for each class on the validation data\n","        plt.subplot(1, 2, 2)\n","        fpr_val = {}\n","        tpr_val = {}\n","        roc_auc_val = {}\n","        for i in range(len(class_labels)):\n","            fpr_val[i], tpr_val[i], _ = roc_curve(y_val, model.predict_proba(X_val)[:, i], pos_label=i)\n","            roc_auc_val[i] = auc(fpr_val[i], tpr_val[i])\n","            plt.plot(fpr_val[i], tpr_val[i], lw=2, label=f'{class_labels[i]} (AUC = {roc_auc_val[i]:0.2f})')\n","        plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')\n","        plt.xlim([0.0, 1.0])\n","        plt.ylim([0.0, 1.05])\n","        plt.xlabel('False Positive Rate')\n","        plt.ylabel('True Positive Rate')\n","        plt.title('ROC Curve - Validation Set')\n","        plt.legend(loc='lower right')\n","        plt.tight_layout()\n","\n","        plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8E9ZHSsDvoJV"},"outputs":[],"source":["rf = RandomForestClassifier(n_estimators = 100, class_weight= 'balanced')\n","rf_model = rf.fit(X_train_tfidf, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8-QSpKBTvoLd"},"outputs":[],"source":["evaluate_model(rf_model, X_train_tfidf, y_train, X_test_tfidf, y_test, X_val_tfidf, y_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NYPP7mnfvoN8"},"outputs":[],"source":["test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"STwGJBQ-Ig2Q"},"outputs":[],"source":["test_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bSw_0Yd7Ig47"},"outputs":[],"source":["filtered_test_labels = test_labels[(test_labels != -1).all(axis=1)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H3h5KNi5IkgK"},"outputs":[],"source":["test_df = pd.merge(test, filtered_test_labels, on='id', how='inner')\n","test_df['target'] = (test_df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) \u003e 0 ).astype(int)\n","test_df = test_df[['comment_text', 'target']]\n","test_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IA_DWfTfIkin"},"outputs":[],"source":["test_df['target'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JnuXy2NHIklv"},"outputs":[],"source":["# Implementing text normalization\n","data_new_norm = pd.DataFrame()\n","\n","data_new_norm['normalized_comment_text'] = test_df['comment_text'].apply(text_normalizer)\n","data_new_norm['target'] = test_df['target']\n","X_new_norm, y_new = data_new_norm['normalized_comment_text'].tolist(), data_new_norm['target'].tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JADSKezaIg7i"},"outputs":[],"source":["# TF-IDF vectorization\n","X_new_tfidf = TfidfVec.transform(X_new_norm)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3bHdmYBfIrq7"},"outputs":[],"source":["y_new_predict = rf_model.predict(X_new_tfidf)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iAj5k451IroI"},"outputs":[],"source":["# Calculate accuracy\n","accuracy = accuracy_score(y_new, y_new_predict)\n","\n","print(f\"New Data Accuracy using RF Model: {accuracy:.4f}\")"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOXpHPo4crd05axgYCJvVYz","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}